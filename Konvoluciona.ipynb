{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read only stvari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "states=[\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
    "    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',\n",
    "    'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
    "    'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n",
    "    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
    "    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia',\n",
    "    'Wisconsin', 'Wyoming'\n",
    "]\n",
    "main_folder=\"..\"\n",
    "fullset=os.path.join(main_folder,'d50States10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisemo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os \n",
    "import cv2\n",
    "from torch.utils.data import Dataset,TensorDataset\n",
    "\n",
    "def obradi(slika):\n",
    "    r=slika[:,:,0]\n",
    "    g=slika[:,:,1]\n",
    "    b=slika[:,:,2]\n",
    "    slika = np.stack([r, g, b], axis=0)\n",
    "    slika=slika.astype(float)\n",
    "    slika/=255\n",
    "    slika=torch.from_numpy(slika)\n",
    "    return slika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa i inicijalizacija seta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LokacijeDataset(Dataset):\n",
    "    def __init__(self,tip):\n",
    "        imena=[] #Ime slike\n",
    "        klase=[] #Broj klase\n",
    "        for i in range(0,50):\n",
    "            put=os.path.join(fullset,states[i])\n",
    "            slike=os.listdir(put)\n",
    "            poc=0\n",
    "            if tip!=\"train\":\n",
    "                poc=2250*4\n",
    "            for j in range(poc,len(slike),4):\n",
    "               # print(j,tip)\n",
    "                if j//4>=2250 and tip==\"train\":\n",
    "                    break\n",
    "                #print(states[i],slike[j],slike[j][:-7])\n",
    "                imena.append(slike[j][:-6])\n",
    "                klase.append(i)\n",
    "        self.klase=klase\n",
    "        self.imena=imena\n",
    "    def __len__(self):\n",
    "        return len(self.klase)\n",
    "    def __getitem__(self,idx):\n",
    "        ime = self.imena[idx]\n",
    "        klasa = self.klase[idx]\n",
    "        #print(klasa,ime)\n",
    "        ret=[]\n",
    "        for i in range(0,360,90):\n",
    "            #print(ime)\n",
    "            trenime=ime+\"_\"+str(i)+\".jpg\"\n",
    "            put=os.path.join(fullset,states[klasa])\n",
    "            put=os.path.join(put,trenime)\n",
    "           # print(put)\n",
    "            slika=cv2.imread(put)\n",
    "            slika=obradi(slika)\n",
    "            ret.append(slika)\n",
    "        del slika\n",
    "        return { \n",
    "            'data':torch.stack(ret),\n",
    "            'label':self.klase[idx]\n",
    "        }\n",
    "\n",
    "train_set = LokacijeDataset(\"train\")\n",
    "test_set= LokacijeDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': tensor([[[[0.5333, 0.5333, 0.5333,  ..., 0.5725, 0.5725, 0.5725],\n",
      "          [0.5333, 0.5333, 0.5333,  ..., 0.5725, 0.5725, 0.5725],\n",
      "          [0.5333, 0.5333, 0.5333,  ..., 0.5725, 0.5725, 0.5725],\n",
      "          ...,\n",
      "          [0.6745, 0.6196, 0.6588,  ..., 0.1843, 0.2627, 0.1804],\n",
      "          [0.7059, 0.6431, 0.5804,  ..., 0.2000, 0.1255, 0.1922],\n",
      "          [0.6471, 0.6039, 0.6549,  ..., 0.1765, 0.2510, 0.1725]],\n",
      "\n",
      "         [[0.4157, 0.4157, 0.4157,  ..., 0.4235, 0.4235, 0.4235],\n",
      "          [0.4157, 0.4157, 0.4157,  ..., 0.4235, 0.4235, 0.4235],\n",
      "          [0.4157, 0.4157, 0.4157,  ..., 0.4235, 0.4235, 0.4235],\n",
      "          ...,\n",
      "          [0.6824, 0.6314, 0.6706,  ..., 0.1922, 0.2706, 0.1882],\n",
      "          [0.7020, 0.6510, 0.5882,  ..., 0.2078, 0.1333, 0.2000],\n",
      "          [0.6431, 0.6000, 0.6627,  ..., 0.1843, 0.2588, 0.1804]],\n",
      "\n",
      "         [[0.2784, 0.2784, 0.2784,  ..., 0.2863, 0.2863, 0.2863],\n",
      "          [0.2784, 0.2784, 0.2784,  ..., 0.2863, 0.2863, 0.2863],\n",
      "          [0.2784, 0.2784, 0.2784,  ..., 0.2863, 0.2863, 0.2863],\n",
      "          ...,\n",
      "          [0.7137, 0.6627, 0.7020,  ..., 0.1961, 0.2745, 0.1922],\n",
      "          [0.7373, 0.6824, 0.6196,  ..., 0.2118, 0.1373, 0.2039],\n",
      "          [0.6784, 0.6353, 0.6941,  ..., 0.1882, 0.2627, 0.1843]]],\n",
      "\n",
      "\n",
      "        [[[0.5647, 0.5647, 0.5647,  ..., 0.5686, 0.5686, 0.5686],\n",
      "          [0.5647, 0.5647, 0.5647,  ..., 0.5686, 0.5686, 0.5686],\n",
      "          [0.5647, 0.5647, 0.5647,  ..., 0.5686, 0.5686, 0.5686],\n",
      "          ...,\n",
      "          [0.6196, 0.6275, 0.6431,  ..., 0.1608, 0.2431, 0.1647],\n",
      "          [0.5882, 0.6275, 0.6196,  ..., 0.1804, 0.1098, 0.1804],\n",
      "          [0.6000, 0.6745, 0.5922,  ..., 0.1569, 0.2392, 0.1647]],\n",
      "\n",
      "         [[0.4235, 0.4235, 0.4235,  ..., 0.4431, 0.4431, 0.4431],\n",
      "          [0.4235, 0.4235, 0.4235,  ..., 0.4431, 0.4431, 0.4431],\n",
      "          [0.4235, 0.4235, 0.4235,  ..., 0.4431, 0.4431, 0.4431],\n",
      "          ...,\n",
      "          [0.6588, 0.6667, 0.6824,  ..., 0.1765, 0.2588, 0.1804],\n",
      "          [0.6353, 0.6745, 0.6588,  ..., 0.1961, 0.1255, 0.1961],\n",
      "          [0.6471, 0.7216, 0.6314,  ..., 0.1725, 0.2549, 0.1804]],\n",
      "\n",
      "         [[0.2824, 0.2824, 0.2824,  ..., 0.3294, 0.3294, 0.3294],\n",
      "          [0.2824, 0.2824, 0.2824,  ..., 0.3294, 0.3294, 0.3294],\n",
      "          [0.2824, 0.2824, 0.2824,  ..., 0.3294, 0.3294, 0.3294],\n",
      "          ...,\n",
      "          [0.6980, 0.7059, 0.7216,  ..., 0.1804, 0.2627, 0.1843],\n",
      "          [0.6745, 0.7137, 0.6980,  ..., 0.2000, 0.1294, 0.2000],\n",
      "          [0.6863, 0.7608, 0.6706,  ..., 0.1765, 0.2588, 0.1843]]],\n",
      "\n",
      "\n",
      "        [[[0.5686, 0.5686, 0.5686,  ..., 0.6275, 0.6275, 0.6275],\n",
      "          [0.5686, 0.5686, 0.5686,  ..., 0.6275, 0.6275, 0.6275],\n",
      "          [0.5686, 0.5686, 0.5686,  ..., 0.6275, 0.6275, 0.6275],\n",
      "          ...,\n",
      "          [0.6157, 0.6078, 0.6000,  ..., 0.1569, 0.2392, 0.1725],\n",
      "          [0.6353, 0.6275, 0.6196,  ..., 0.1765, 0.1059, 0.1922],\n",
      "          [0.6078, 0.6118, 0.6157,  ..., 0.1529, 0.2314, 0.1765]],\n",
      "\n",
      "         [[0.4471, 0.4471, 0.4471,  ..., 0.5725, 0.5725, 0.5725],\n",
      "          [0.4471, 0.4471, 0.4471,  ..., 0.5725, 0.5725, 0.5725],\n",
      "          [0.4471, 0.4471, 0.4471,  ..., 0.5725, 0.5725, 0.5725],\n",
      "          ...,\n",
      "          [0.6471, 0.6392, 0.6314,  ..., 0.1725, 0.2549, 0.1882],\n",
      "          [0.6667, 0.6588, 0.6510,  ..., 0.1922, 0.1216, 0.2078],\n",
      "          [0.6392, 0.6431, 0.6471,  ..., 0.1686, 0.2471, 0.1922]],\n",
      "\n",
      "         [[0.3255, 0.3255, 0.3255,  ..., 0.4863, 0.4863, 0.4863],\n",
      "          [0.3255, 0.3255, 0.3255,  ..., 0.4863, 0.4863, 0.4863],\n",
      "          [0.3255, 0.3255, 0.3255,  ..., 0.4863, 0.4863, 0.4863],\n",
      "          ...,\n",
      "          [0.6745, 0.6667, 0.6588,  ..., 0.1765, 0.2588, 0.1922],\n",
      "          [0.6941, 0.6863, 0.6784,  ..., 0.1961, 0.1255, 0.2118],\n",
      "          [0.6667, 0.6706, 0.6745,  ..., 0.1725, 0.2510, 0.1961]]],\n",
      "\n",
      "\n",
      "        [[[0.6235, 0.6235, 0.6235,  ..., 0.5451, 0.5451, 0.5451],\n",
      "          [0.6235, 0.6235, 0.6235,  ..., 0.5451, 0.5451, 0.5451],\n",
      "          [0.6235, 0.6235, 0.6235,  ..., 0.5451, 0.5451, 0.5451],\n",
      "          ...,\n",
      "          [0.5804, 0.5804, 0.6157,  ..., 0.2039, 0.2392, 0.1686],\n",
      "          [0.5804, 0.5843, 0.6157,  ..., 0.1216, 0.0745, 0.2706],\n",
      "          [0.6314, 0.6235, 0.5608,  ..., 0.2588, 0.2471, 0.1843]],\n",
      "\n",
      "         [[0.5608, 0.5608, 0.5608,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          [0.5608, 0.5608, 0.5608,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          [0.5608, 0.5608, 0.5608,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          ...,\n",
      "          [0.6196, 0.6196, 0.6549,  ..., 0.2118, 0.2471, 0.1765],\n",
      "          [0.6275, 0.6314, 0.6549,  ..., 0.1294, 0.0824, 0.2784],\n",
      "          [0.6784, 0.6706, 0.6000,  ..., 0.2667, 0.2549, 0.1922]],\n",
      "\n",
      "         [[0.4706, 0.4706, 0.4706,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          [0.4706, 0.4706, 0.4706,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          [0.4706, 0.4706, 0.4706,  ..., 0.2902, 0.2902, 0.2902],\n",
      "          ...,\n",
      "          [0.6588, 0.6588, 0.6941,  ..., 0.2157, 0.2510, 0.1804],\n",
      "          [0.6667, 0.6706, 0.6941,  ..., 0.1333, 0.0863, 0.2824],\n",
      "          [0.7176, 0.7098, 0.6392,  ..., 0.2706, 0.2588, 0.1961]]]],\n",
      "       dtype=torch.float64), 'label': 49}\n"
     ]
    }
   ],
   "source": [
    "print(test_set[12499])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicijalizacija mreze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Mreza (nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        self.konvolucija=nn.Sequential(\n",
    "            nn.Conv2d (3, 12, 3, 1, 1), #256\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d (12, 24, 3, 1, 1),#128\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d (24, 48, 3, 1, 1),#64\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d (48, 96, 3, 1, 1),#32\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d (96, 192, 3, 1, 1),#16\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d (192, 384, 3, 1, 1),#8\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.povezan=nn.Sequential(\n",
    "            nn.Linear (24576, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 450),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(450, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward (self, x):\n",
    "       # print(x.size())\n",
    "        x=x.type(torch.float)\n",
    "        a=self.konvolucija(x[:,0,:,:,:])\n",
    "        b=self.konvolucija(x[:,1,:,:,:])\n",
    "        c=self.konvolucija(x[:,2,:,:,:])\n",
    "        d=self.konvolucija(x[:,3,:,:,:])\n",
    "        tens=torch.cat([a,b,c,d],1)\n",
    "        del a,b,c,d\n",
    "        return self.povezan(tens)\n",
    "model = Mreza()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podesavanje uredjaja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totensor(y):\n",
    "    ret=np.zeros((len(y),50))\n",
    "   # print(y[0])\n",
    "   # print(y[0][0])\n",
    "   # print(y[0][0].item())\n",
    "    print(y)\n",
    "    for i in range(0,len(y)):\n",
    "        ret[i][round(y[i].item())]=1\n",
    "    return torch.Tensor(ret)\n",
    "\n",
    "def get_accuracy(y_hat,y):\n",
    "    #print(y_hat,y)\n",
    "    y_hat = y_hat.detach().numpy()\n",
    "    y=y.numpy()\n",
    "    y_hat=np.argmax(y_hat,axis=1)\n",
    "    corrects=(y_hat==y).sum()\n",
    "    accuracy=100.0*corrects/y_hat.shape[0]\n",
    "    return accuracy.item()\n",
    "\n",
    "def test(model,sample):\n",
    "    #print(sample)\n",
    "    rez=model(sample.reshape(1,4,3,256,256))\n",
    "    #print(rez)\n",
    "    rez=rez.detach().numpy()\n",
    "    #print(rez)\n",
    "    poz=0\n",
    "    for i in range(1,len(rez)):\n",
    "        if(rez[i]>rez[poz]):\n",
    "            poz=i\n",
    "    return poz\n",
    "\n",
    "def validation(model):\n",
    "    model.to(\"cpu\")\n",
    "    kol=0\n",
    "    for i in range(0,len(test_set)):\n",
    "        out=test(model,test_set[i][0])\n",
    "        ans=test_set[i][1][0].item()\n",
    "        if round(ans)==out:\n",
    "            kol+=1\n",
    "    return kol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1048576000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m  y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m  y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     20\u001b[0m  loss \u001b[39m=\u001b[39m loss_funk(y_hat, totensor(y))\n\u001b[0;32m     21\u001b[0m  optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m, in \u001b[0;36mMreza.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m a\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkonvolucija(x[:,\u001b[39m0\u001b[39m,:,:,:])\n\u001b[0;32m     52\u001b[0m b\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkonvolucija(x[:,\u001b[39m1\u001b[39m,:,:,:])\n\u001b[1;32m---> 53\u001b[0m c\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkonvolucija(x[:,\u001b[39m2\u001b[39;49m,:,:,:])\n\u001b[0;32m     54\u001b[0m d\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkonvolucija(x[:,\u001b[39m3\u001b[39m,:,:,:])\n\u001b[0;32m     55\u001b[0m tens\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat([a,b,c,d],\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1048576000 bytes."
     ]
    }
   ],
   "source": [
    "broj_epoha = 1\n",
    "batch_velicina=250\n",
    "loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_velicina,shuffle=True,num_workers=0,drop_last=True,pin_memory=True)\n",
    "\n",
    "loss_funk = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "for epoha in range(1,broj_epoha+1):\n",
    "    train_running_loss =0.0\n",
    "    train_acc=0.0\n",
    "    model = model.train()\n",
    "\n",
    "    for batch_num, lokacije in enumerate(loader):\n",
    "            x=lokacije['data']\n",
    "            y=lokacije['label']\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "           # print(x)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_funk(y_hat, totensor(y))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_accuracy(y_hat=y_hat, y=y)\n",
    "            del y_hat\n",
    "            del x\n",
    "            del y \n",
    "            del loss\n",
    "\n",
    "        \n",
    "    epoch_loss = train_running_loss / batch_num\n",
    "    epoch_acc = train_acc / batch_num\n",
    "        \n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f | Validation Accuracy: %.2f' \\\n",
    "          %(epoha, epoch_loss, epoch_acc, validation(model)/len(test_set)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.6431, 0.6392, 0.6314,  ..., 0.6000, 0.6000, 0.6000],\n",
      "           [0.6431, 0.6392, 0.6314,  ..., 0.6000, 0.6000, 0.6000],\n",
      "           [0.6392, 0.6353, 0.6314,  ..., 0.6000, 0.6000, 0.6000],\n",
      "           ...,\n",
      "           [0.2588, 0.2353, 0.2745,  ..., 0.0784, 0.1569, 0.0863],\n",
      "           [0.2196, 0.1961, 0.2745,  ..., 0.1176, 0.0627, 0.1608],\n",
      "           [0.2706, 0.1059, 0.2196,  ..., 0.1020, 0.1647, 0.0745]],\n",
      "\n",
      "          [[0.4863, 0.4824, 0.4745,  ..., 0.4275, 0.4275, 0.4275],\n",
      "           [0.4863, 0.4824, 0.4745,  ..., 0.4275, 0.4275, 0.4275],\n",
      "           [0.4824, 0.4784, 0.4745,  ..., 0.4275, 0.4275, 0.4275],\n",
      "           ...,\n",
      "           [0.2902, 0.2784, 0.3255,  ..., 0.0902, 0.1686, 0.0980],\n",
      "           [0.2471, 0.2314, 0.3255,  ..., 0.1294, 0.0745, 0.1725],\n",
      "           [0.2980, 0.1412, 0.2627,  ..., 0.1137, 0.1765, 0.0863]],\n",
      "\n",
      "          [[0.3216, 0.3176, 0.3098,  ..., 0.2824, 0.2824, 0.2824],\n",
      "           [0.3216, 0.3176, 0.3098,  ..., 0.2824, 0.2824, 0.2824],\n",
      "           [0.3176, 0.3137, 0.3098,  ..., 0.2824, 0.2824, 0.2824],\n",
      "           ...,\n",
      "           [0.4039, 0.3882, 0.4275,  ..., 0.1059, 0.1843, 0.1137],\n",
      "           [0.3529, 0.3373, 0.4275,  ..., 0.1451, 0.0902, 0.1882],\n",
      "           [0.4039, 0.2471, 0.3686,  ..., 0.1294, 0.1922, 0.1020]]],\n",
      "\n",
      "\n",
      "         [[[0.5843, 0.5843, 0.5843,  ..., 0.3961, 0.4980, 0.2314],\n",
      "           [0.5843, 0.5843, 0.5843,  ..., 0.4275, 0.5647, 0.3412],\n",
      "           [0.5843, 0.5843, 0.5882,  ..., 0.3961, 0.5608, 0.4980],\n",
      "           ...,\n",
      "           [0.3843, 0.3922, 0.3961,  ..., 0.0275, 0.1098, 0.0471],\n",
      "           [0.4000, 0.3922, 0.3922,  ..., 0.0706, 0.0157, 0.1294],\n",
      "           [0.4118, 0.3922, 0.3922,  ..., 0.0549, 0.1176, 0.0471]],\n",
      "\n",
      "          [[0.4196, 0.4196, 0.4196,  ..., 0.3765, 0.4627, 0.1961],\n",
      "           [0.4196, 0.4196, 0.4196,  ..., 0.3961, 0.5294, 0.3059],\n",
      "           [0.4196, 0.4196, 0.4235,  ..., 0.3647, 0.5255, 0.4549],\n",
      "           ...,\n",
      "           [0.4275, 0.4353, 0.4471,  ..., 0.0353, 0.1176, 0.0549],\n",
      "           [0.4314, 0.4314, 0.4431,  ..., 0.0784, 0.0235, 0.1373],\n",
      "           [0.4431, 0.4314, 0.4314,  ..., 0.0627, 0.1255, 0.0549]],\n",
      "\n",
      "          [[0.2745, 0.2745, 0.2745,  ..., 0.3725, 0.4510, 0.1843],\n",
      "           [0.2745, 0.2745, 0.2745,  ..., 0.3961, 0.5176, 0.2941],\n",
      "           [0.2745, 0.2745, 0.2784,  ..., 0.3647, 0.5137, 0.4392],\n",
      "           ...,\n",
      "           [0.4824, 0.4902, 0.5020,  ..., 0.0392, 0.1216, 0.0588],\n",
      "           [0.4980, 0.4980, 0.5059,  ..., 0.0824, 0.0275, 0.1412],\n",
      "           [0.5098, 0.4980, 0.4980,  ..., 0.0667, 0.1294, 0.0588]]],\n",
      "\n",
      "\n",
      "         [[[0.2745, 0.3686, 0.3922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           [0.2431, 0.5137, 0.6118,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           [0.1843, 0.4275, 0.6078,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           ...,\n",
      "           [0.2667, 0.2824, 0.3059,  ..., 0.0902, 0.1725, 0.1020],\n",
      "           [0.2706, 0.2863, 0.3020,  ..., 0.1333, 0.0863, 0.1804],\n",
      "           [0.2784, 0.2941, 0.2980,  ..., 0.1216, 0.1882, 0.0980]],\n",
      "\n",
      "          [[0.2824, 0.3843, 0.4078,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           [0.2510, 0.5294, 0.6275,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           [0.1922, 0.4431, 0.6235,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           ...,\n",
      "           [0.2784, 0.3059, 0.3176,  ..., 0.1020, 0.1843, 0.1137],\n",
      "           [0.2902, 0.3137, 0.3255,  ..., 0.1451, 0.0980, 0.1922],\n",
      "           [0.3059, 0.3216, 0.3294,  ..., 0.1333, 0.2000, 0.1098]],\n",
      "\n",
      "          [[0.2863, 0.3882, 0.4275,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           [0.2549, 0.5333, 0.6471,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           [0.1961, 0.4471, 0.6431,  ..., 1.0000, 1.0000, 1.0000],\n",
      "           ...,\n",
      "           [0.3333, 0.3569, 0.3725,  ..., 0.1176, 0.2000, 0.1294],\n",
      "           [0.3490, 0.3725, 0.3765,  ..., 0.1608, 0.1137, 0.2078],\n",
      "           [0.3647, 0.3804, 0.3804,  ..., 0.1490, 0.2157, 0.1255]]],\n",
      "\n",
      "\n",
      "         [[[0.9961, 0.9961, 0.9961,  ..., 0.6471, 0.6471, 0.6431],\n",
      "           [0.9961, 0.9961, 0.9961,  ..., 0.6471, 0.6471, 0.6431],\n",
      "           [0.9961, 0.9961, 0.9961,  ..., 0.6510, 0.6471, 0.6471],\n",
      "           ...,\n",
      "           [0.4392, 0.4235, 0.4196,  ..., 0.0824, 0.1725, 0.1333],\n",
      "           [0.4353, 0.4157, 0.4118,  ..., 0.0863, 0.0235, 0.1373],\n",
      "           [0.4392, 0.4235, 0.4196,  ..., 0.1451, 0.1882, 0.1098]],\n",
      "\n",
      "          [[0.9961, 0.9961, 0.9961,  ..., 0.4824, 0.4824, 0.4784],\n",
      "           [0.9961, 0.9961, 0.9961,  ..., 0.4824, 0.4824, 0.4784],\n",
      "           [0.9961, 0.9961, 0.9961,  ..., 0.4863, 0.4824, 0.4824],\n",
      "           ...,\n",
      "           [0.4745, 0.4588, 0.4549,  ..., 0.1333, 0.2235, 0.1843],\n",
      "           [0.4784, 0.4588, 0.4549,  ..., 0.1451, 0.0824, 0.1961],\n",
      "           [0.4824, 0.4667, 0.4627,  ..., 0.2039, 0.2471, 0.1686]],\n",
      "\n",
      "          [[0.9961, 0.9961, 0.9961,  ..., 0.3373, 0.3373, 0.3333],\n",
      "           [0.9961, 0.9961, 0.9961,  ..., 0.3373, 0.3373, 0.3333],\n",
      "           [0.9961, 0.9961, 0.9961,  ..., 0.3412, 0.3373, 0.3373],\n",
      "           ...,\n",
      "           [0.5529, 0.5373, 0.5333,  ..., 0.1647, 0.2549, 0.2157],\n",
      "           [0.5569, 0.5373, 0.5333,  ..., 0.1804, 0.1176, 0.2314],\n",
      "           [0.5608, 0.5451, 0.5412,  ..., 0.2392, 0.2824, 0.2039]]]]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2328, 0.0000, 0.0000, 0.0354, 0.0538, 0.0000, 0.0035, 0.0000, 0.0000,\n",
       "         0.0467, 0.2303, 0.0000, 0.0000, 0.0956, 0.0212, 0.0000, 0.0538, 0.0000,\n",
       "         0.0000, 0.0000, 0.0731, 0.0000, 0.0000, 0.0000, 0.1463, 0.0000, 0.1336,\n",
       "         0.0000, 0.0000, 0.0000, 0.0241, 0.0000, 0.2073, 0.0273, 0.1420, 0.1940,\n",
       "         0.0000, 0.0000, 0.0000, 0.0955, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
       "         0.0000, 0.0000, 0.1352, 0.0000, 0.0942]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=test_set[0]['data']\n",
    "a=a.reshape(1,4,3,256,256)\n",
    "print(a)\n",
    "model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(test_set)):\n\u001b[0;32m      2\u001b[0m     a\u001b[39m=\u001b[39mtest_set[i][\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mif\u001b[39;00m(test(model,a)\u001b[39m==\u001b[39mtest_set[i][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m      5\u001b[0m \u001b[39m#print(test(model,test_set[2000]['data']),test_set[2000]['label'])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mLokacijeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m     ret\u001b[39m.\u001b[39mappend(slika)\n\u001b[0;32m     36\u001b[0m \u001b[39mdel\u001b[39;00m slika\n\u001b[0;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m { \n\u001b[1;32m---> 38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:torch\u001b[39m.\u001b[39;49mstack(ret),\n\u001b[0;32m     39\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mklase[idx]\n\u001b[0;32m     40\u001b[0m }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,len(test_set)):\n",
    "    a=test_set[i]['data']\n",
    "    if(test(model,a)==test_set[i]['label']):\n",
    "        print(i)\n",
    "#print(test(model,test_set[2000]['data']),test_set[2000]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
