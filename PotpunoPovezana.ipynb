{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ucitavamo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "(tensor([0.3936, 0.4364, 0.4423,  ..., 0.0000, 0.0000, 0.0899]), tensor([22.]))\n",
      "tensor(0.4423)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ulaz= np.loadtxt('Dataset.txt')\n",
    "podaci=ulaz[:,1:]\n",
    "#print(podaci)\n",
    "b=np.max(podaci,axis=0)\n",
    "podaci=podaci/b\n",
    "#print(podaci)\n",
    "#print(np.max(podaci),np.min(podaci))\n",
    "obelezja=ulaz[:,:1]\n",
    "#print(obelezja)\n",
    "podaci=torch.Tensor(podaci)\n",
    "obelezja=torch.Tensor(obelezja)\n",
    "\n",
    "dataset=TensorDataset(podaci,obelezja)\n",
    "train_set,test_set = torch.utils.data.random_split(dataset,[0.9,0.1])\n",
    "\n",
    "print(len(test_set))\n",
    "print(test_set[0])\n",
    "print(test_set[0][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicijalizacija mreze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Mreza (nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        self.act=nn.Sequential(\n",
    "            nn.Linear(1564,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward (self, x):\n",
    "        return self.act(x)\n",
    "model = Mreza()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podesavanje uredjaja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testiranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totensor(y):\n",
    "    ret=np.zeros((len(y),50))\n",
    "   # print(y[0])\n",
    "   # print(y[0][0])\n",
    "   # print(y[0][0].item())\n",
    "    for i in range(0,len(y)):\n",
    "        ret[i][round(y[i][0].item())]=1\n",
    "    return torch.Tensor(ret)\n",
    "\n",
    "def get_accuracy(y_hat,y):\n",
    "    #print(y_hat,y)\n",
    "    y_hat = y_hat.detach().numpy()\n",
    "    y=y.detach().numpy()\n",
    "    y_hat=np.argmax(y_hat,axis=0)\n",
    "    corrects=(y_hat==y).sum()\n",
    "    accuracy=100.0*corrects/y_hat.shape[0]\n",
    "    return accuracy.item()\n",
    "\n",
    "def test(model,sample):\n",
    "    rez=model(sample)\n",
    "    #print(rez)\n",
    "    rez=rez.detach().numpy()\n",
    "    #print(rez)\n",
    "    poz=0\n",
    "    for i in range(1,len(rez)):\n",
    "        if(rez[i]>rez[poz]):\n",
    "            poz=i\n",
    "    return poz\n",
    "\n",
    "def validation(model):\n",
    "    model.to(\"cpu\")\n",
    "    kol=0\n",
    "    for i in range(0,len(test_set)):\n",
    "        out=test(model,test_set[i][0])\n",
    "        ans=test_set[i][1][0].item()\n",
    "        if round(ans)==out:\n",
    "            kol+=1\n",
    "    return kol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 3.7698 | Train Accuracy: 155.66 | Validation Accuracy: 11.37\n",
      "Epoch: 2 | Loss: 3.6381 | Train Accuracy: 155.19 | Validation Accuracy: 13.63\n",
      "Epoch: 3 | Loss: 3.5728 | Train Accuracy: 154.24 | Validation Accuracy: 14.94\n",
      "Epoch: 4 | Loss: 3.5265 | Train Accuracy: 154.55 | Validation Accuracy: 15.64\n",
      "Epoch: 5 | Loss: 3.4945 | Train Accuracy: 154.33 | Validation Accuracy: 15.56\n",
      "Epoch: 6 | Loss: 3.4659 | Train Accuracy: 153.59 | Validation Accuracy: 16.38\n",
      "Epoch: 7 | Loss: 3.4427 | Train Accuracy: 154.46 | Validation Accuracy: 16.73\n",
      "Epoch: 8 | Loss: 3.4232 | Train Accuracy: 154.64 | Validation Accuracy: 17.29\n",
      "Epoch: 9 | Loss: 3.4042 | Train Accuracy: 154.25 | Validation Accuracy: 16.88\n",
      "Epoch: 10 | Loss: 3.3876 | Train Accuracy: 154.52 | Validation Accuracy: 17.17\n",
      "Epoch: 11 | Loss: 3.3739 | Train Accuracy: 154.35 | Validation Accuracy: 17.14\n",
      "Epoch: 12 | Loss: 3.3561 | Train Accuracy: 154.79 | Validation Accuracy: 17.16\n",
      "Epoch: 13 | Loss: 3.3430 | Train Accuracy: 153.80 | Validation Accuracy: 17.43\n",
      "Epoch: 14 | Loss: 3.3306 | Train Accuracy: 154.33 | Validation Accuracy: 17.60\n",
      "Epoch: 15 | Loss: 3.3178 | Train Accuracy: 153.87 | Validation Accuracy: 17.48\n",
      "Epoch: 16 | Loss: 3.3039 | Train Accuracy: 153.55 | Validation Accuracy: 17.61\n",
      "Epoch: 17 | Loss: 3.2941 | Train Accuracy: 153.93 | Validation Accuracy: 17.60\n",
      "Epoch: 18 | Loss: 3.2844 | Train Accuracy: 155.19 | Validation Accuracy: 18.22\n",
      "Epoch: 19 | Loss: 3.2710 | Train Accuracy: 153.89 | Validation Accuracy: 18.12\n",
      "Epoch: 20 | Loss: 3.2612 | Train Accuracy: 153.60 | Validation Accuracy: 17.59\n",
      "Epoch: 21 | Loss: 3.2483 | Train Accuracy: 154.55 | Validation Accuracy: 17.60\n",
      "Epoch: 22 | Loss: 3.2400 | Train Accuracy: 154.05 | Validation Accuracy: 18.22\n",
      "Epoch: 23 | Loss: 3.2304 | Train Accuracy: 154.43 | Validation Accuracy: 17.93\n",
      "Epoch: 24 | Loss: 3.2206 | Train Accuracy: 155.13 | Validation Accuracy: 18.05\n",
      "Epoch: 25 | Loss: 3.2127 | Train Accuracy: 154.36 | Validation Accuracy: 17.91\n",
      "Epoch: 26 | Loss: 3.2007 | Train Accuracy: 154.93 | Validation Accuracy: 17.64\n",
      "Epoch: 27 | Loss: 3.1922 | Train Accuracy: 153.43 | Validation Accuracy: 17.63\n",
      "Epoch: 28 | Loss: 3.1848 | Train Accuracy: 154.38 | Validation Accuracy: 17.41\n",
      "Epoch: 29 | Loss: 3.1787 | Train Accuracy: 153.79 | Validation Accuracy: 17.50\n",
      "Epoch: 30 | Loss: 3.1689 | Train Accuracy: 154.43 | Validation Accuracy: 17.70\n",
      "Epoch: 31 | Loss: 3.1631 | Train Accuracy: 154.33 | Validation Accuracy: 18.11\n",
      "Epoch: 32 | Loss: 3.1531 | Train Accuracy: 154.25 | Validation Accuracy: 17.76\n",
      "Epoch: 33 | Loss: 3.1466 | Train Accuracy: 153.42 | Validation Accuracy: 17.82\n",
      "Epoch: 34 | Loss: 3.1386 | Train Accuracy: 153.84 | Validation Accuracy: 17.70\n",
      "Epoch: 35 | Loss: 3.1317 | Train Accuracy: 155.02 | Validation Accuracy: 17.67\n",
      "Epoch: 36 | Loss: 3.1227 | Train Accuracy: 155.03 | Validation Accuracy: 17.86\n",
      "Epoch: 37 | Loss: 3.1172 | Train Accuracy: 154.08 | Validation Accuracy: 17.57\n",
      "Epoch: 38 | Loss: 3.1092 | Train Accuracy: 154.70 | Validation Accuracy: 17.71\n",
      "Epoch: 39 | Loss: 3.1045 | Train Accuracy: 155.19 | Validation Accuracy: 17.74\n",
      "Epoch: 40 | Loss: 3.0952 | Train Accuracy: 154.21 | Validation Accuracy: 17.36\n",
      "Epoch: 41 | Loss: 3.0919 | Train Accuracy: 154.50 | Validation Accuracy: 17.51\n",
      "Epoch: 42 | Loss: 3.0849 | Train Accuracy: 154.46 | Validation Accuracy: 17.26\n",
      "Epoch: 43 | Loss: 3.0747 | Train Accuracy: 154.80 | Validation Accuracy: 17.64\n",
      "Epoch: 44 | Loss: 3.0716 | Train Accuracy: 154.29 | Validation Accuracy: 17.48\n",
      "Epoch: 45 | Loss: 3.0665 | Train Accuracy: 154.26 | Validation Accuracy: 17.42\n",
      "Epoch: 46 | Loss: 3.0577 | Train Accuracy: 155.57 | Validation Accuracy: 17.36\n",
      "Epoch: 47 | Loss: 3.0525 | Train Accuracy: 154.70 | Validation Accuracy: 17.30\n",
      "Epoch: 48 | Loss: 3.0463 | Train Accuracy: 153.57 | Validation Accuracy: 16.76\n",
      "Epoch: 49 | Loss: 3.0413 | Train Accuracy: 154.49 | Validation Accuracy: 17.07\n",
      "Epoch: 50 | Loss: 3.0362 | Train Accuracy: 154.34 | Validation Accuracy: 17.43\n"
     ]
    }
   ],
   "source": [
    "broj_epoha = 50\n",
    "batch_velicina=100\n",
    "loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_velicina,shuffle=True,num_workers=0,drop_last=True,pin_memory=True)\n",
    "loss_funk = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "for epoha in range(1,broj_epoha+1):\n",
    "    train_running_loss =0.0\n",
    "    train_acc=0.0\n",
    "    model = model.train()\n",
    "\n",
    "    for batch_num, (x, y) in enumerate(loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            #print(y)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_funk(y_hat, totensor(y))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_accuracy(y_hat=y_hat, y=y)\n",
    "            del y_hat\n",
    "\n",
    "        \n",
    "    epoch_loss = train_running_loss / batch_num\n",
    "    epoch_acc = train_acc / batch_num\n",
    "        \n",
    "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f | Validation Accuracy: %.2f' \\\n",
    "          %(epoha, epoch_loss, epoch_acc, validation(model)/len(test_set)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794\n",
      "45\n",
      "47\n",
      "57\n",
      "95\n",
      "120\n",
      "135\n",
      "137\n",
      "138\n",
      "158\n",
      "200\n",
      "210\n",
      "215\n",
      "224\n",
      "248\n",
      "252\n",
      "262\n",
      "278\n",
      "332\n",
      "364\n",
      "408\n",
      "411\n",
      "424\n",
      "427\n",
      "449\n",
      "456\n",
      "460\n",
      "470\n",
      "479\n",
      "484\n",
      "487\n",
      "488\n",
      "492\n",
      "542\n",
      "545\n",
      "554\n",
      "570\n",
      "575\n",
      "613\n",
      "616\n",
      "624\n",
      "644\n",
      "655\n",
      "656\n",
      "685\n",
      "694\n",
      "704\n",
      "706\n",
      "709\n",
      "716\n",
      "728\n",
      "735\n",
      "758\n",
      "762\n",
      "765\n",
      "786\n",
      "822\n",
      "834\n",
      "847\n",
      "849\n",
      "858\n",
      "868\n",
      "884\n",
      "901\n",
      "909\n",
      "912\n",
      "922\n",
      "925\n",
      "945\n",
      "959\n",
      "966\n",
      "982\n",
      "991\n",
      "998\n",
      "999\n",
      "1025\n",
      "1026\n",
      "1065\n",
      "1067\n",
      "1089\n",
      "1090\n",
      "1095\n",
      "1107\n",
      "1111\n",
      "1118\n",
      "1120\n",
      "1143\n",
      "1147\n",
      "1151\n",
      "1226\n",
      "1228\n",
      "1273\n",
      "1277\n",
      "1279\n",
      "1287\n",
      "1292\n",
      "1300\n",
      "1301\n",
      "1307\n",
      "1316\n",
      "1337\n",
      "1342\n",
      "1368\n",
      "1373\n",
      "1387\n",
      "1436\n",
      "1446\n",
      "1462\n",
      "1511\n",
      "1524\n",
      "1534\n",
      "1537\n",
      "1546\n",
      "1616\n",
      "1650\n",
      "1668\n",
      "1703\n",
      "1708\n",
      "1713\n",
      "1761\n",
      "1762\n",
      "1781\n",
      "1784\n",
      "1791\n",
      "1813\n",
      "1824\n",
      "1825\n",
      "1828\n",
      "1875\n",
      "1881\n",
      "1893\n",
      "1896\n",
      "1900\n",
      "1917\n",
      "1930\n",
      "1960\n",
      "1976\n",
      "1993\n",
      "1999\n",
      "2011\n",
      "2012\n",
      "2016\n",
      "2025\n",
      "2039\n",
      "2049\n",
      "2075\n",
      "2089\n",
      "2112\n",
      "2118\n",
      "2134\n",
      "2177\n",
      "2197\n",
      "2243\n",
      "2281\n",
      "2288\n",
      "2293\n",
      "2334\n",
      "2341\n",
      "2349\n",
      "2374\n",
      "2394\n",
      "2396\n",
      "2407\n",
      "2459\n",
      "2478\n",
      "2482\n",
      "2494\n",
      "2502\n",
      "2534\n",
      "2539\n",
      "2540\n",
      "2557\n",
      "2561\n",
      "2572\n",
      "2593\n",
      "2601\n",
      "2617\n",
      "2633\n",
      "2640\n",
      "2675\n",
      "2739\n",
      "2755\n",
      "2770\n",
      "2791\n",
      "2808\n",
      "2814\n",
      "2824\n",
      "2827\n",
      "2845\n",
      "2850\n",
      "2863\n",
      "2888\n",
      "2909\n",
      "2916\n",
      "2954\n",
      "2985\n",
      "3015\n",
      "3019\n",
      "3023\n",
      "3071\n",
      "3082\n",
      "3085\n",
      "3103\n",
      "3134\n",
      "3187\n",
      "3197\n",
      "3208\n",
      "3215\n",
      "3226\n",
      "3232\n",
      "3237\n",
      "3260\n",
      "3262\n",
      "3267\n",
      "3286\n",
      "3316\n",
      "3318\n",
      "3326\n",
      "3347\n",
      "3348\n",
      "3350\n",
      "3352\n",
      "3369\n",
      "3403\n",
      "3415\n",
      "3442\n",
      "3455\n",
      "3462\n",
      "3499\n",
      "3506\n",
      "3514\n",
      "3520\n",
      "3529\n",
      "3533\n",
      "3556\n",
      "3585\n",
      "3597\n",
      "3644\n",
      "3680\n",
      "3710\n",
      "3717\n",
      "3720\n",
      "3725\n",
      "3735\n",
      "3743\n",
      "3752\n",
      "3766\n",
      "3773\n",
      "3797\n",
      "3813\n",
      "3814\n",
      "3825\n",
      "3827\n",
      "3836\n",
      "3841\n",
      "3843\n",
      "3855\n",
      "3862\n",
      "3890\n",
      "3903\n",
      "3912\n",
      "3921\n",
      "3923\n",
      "3956\n",
      "3968\n",
      "3977\n",
      "3985\n",
      "3993\n",
      "3999\n",
      "4002\n",
      "4029\n",
      "4038\n",
      "4055\n",
      "4057\n",
      "4068\n",
      "4070\n",
      "4139\n",
      "4163\n",
      "4195\n",
      "4206\n",
      "4226\n",
      "4249\n",
      "4259\n",
      "4297\n",
      "4305\n",
      "4360\n",
      "4435\n",
      "4440\n",
      "4442\n",
      "4504\n",
      "4505\n",
      "4512\n",
      "4525\n",
      "4534\n",
      "4535\n",
      "4546\n",
      "4548\n",
      "4550\n",
      "4569\n",
      "4574\n",
      "4583\n",
      "4593\n",
      "4599\n",
      "4611\n",
      "4616\n",
      "4633\n",
      "4634\n",
      "4646\n",
      "4662\n",
      "4682\n",
      "4683\n",
      "4718\n",
      "4743\n",
      "4744\n",
      "4763\n",
      "4798\n",
      "4828\n",
      "4836\n",
      "4847\n",
      "4860\n",
      "4865\n",
      "4868\n",
      "4901\n",
      "4911\n",
      "4923\n",
      "4940\n",
      "4945\n",
      "5010\n",
      "5035\n",
      "5040\n",
      "5097\n",
      "5146\n",
      "5181\n",
      "5195\n",
      "5206\n",
      "5211\n",
      "5217\n",
      "5231\n",
      "5242\n",
      "5244\n",
      "5259\n",
      "5276\n",
      "5289\n",
      "5292\n",
      "5349\n",
      "5361\n",
      "5372\n",
      "5373\n",
      "5383\n",
      "5390\n",
      "5402\n",
      "5404\n",
      "5433\n",
      "5440\n",
      "5441\n",
      "5451\n",
      "5462\n",
      "5478\n",
      "5495\n",
      "5518\n",
      "5567\n",
      "5578\n",
      "5605\n",
      "5632\n",
      "5641\n",
      "5666\n",
      "5672\n",
      "5674\n",
      "5675\n",
      "5683\n",
      "5688\n",
      "5696\n",
      "5717\n",
      "5725\n",
      "5731\n",
      "5744\n",
      "5751\n",
      "5752\n",
      "5754\n",
      "5767\n",
      "5784\n",
      "5795\n",
      "5797\n",
      "5862\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5915\n",
      "5918\n",
      "5937\n",
      "5939\n",
      "5940\n",
      "5953\n",
      "5956\n",
      "5962\n",
      "5963\n",
      "5976\n",
      "6007\n",
      "6043\n",
      "6047\n",
      "6077\n",
      "6086\n",
      "6092\n",
      "6115\n",
      "6127\n",
      "6130\n",
      "6142\n",
      "6168\n",
      "6225\n",
      "6235\n",
      "6241\n",
      "6250\n",
      "6255\n",
      "6279\n",
      "6287\n",
      "6295\n",
      "6307\n",
      "6308\n",
      "6330\n",
      "6378\n",
      "6384\n",
      "6441\n",
      "6446\n",
      "6456\n",
      "6460\n",
      "6480\n",
      "6486\n",
      "6502\n",
      "6520\n",
      "6522\n",
      "6534\n",
      "6538\n",
      "6560\n",
      "6576\n",
      "6577\n",
      "6600\n",
      "6619\n",
      "6638\n",
      "6640\n",
      "6724\n",
      "6730\n",
      "6740\n",
      "6770\n",
      "6773\n",
      "6796\n",
      "6799\n",
      "6805\n",
      "6813\n",
      "6815\n",
      "6822\n",
      "6834\n",
      "6855\n",
      "6856\n",
      "6862\n",
      "6870\n",
      "6873\n",
      "6907\n",
      "6918\n",
      "6925\n",
      "6931\n",
      "6968\n",
      "7008\n",
      "7054\n",
      "7062\n",
      "7096\n",
      "7132\n",
      "7177\n",
      "7182\n",
      "7184\n",
      "7197\n",
      "7259\n",
      "7263\n",
      "7297\n",
      "7301\n",
      "7321\n",
      "7337\n",
      "7346\n",
      "7376\n",
      "7408\n",
      "7420\n",
      "7437\n",
      "7488\n",
      "7510\n",
      "7531\n",
      "7552\n",
      "7571\n",
      "7582\n",
      "7595\n",
      "7609\n",
      "7622\n",
      "7641\n",
      "7668\n",
      "7674\n",
      "7680\n",
      "7704\n",
      "7711\n",
      "7715\n",
      "7730\n",
      "7744\n",
      "7770\n",
      "7790\n",
      "7799\n",
      "7806\n",
      "7823\n",
      "7839\n",
      "7842\n",
      "7845\n",
      "7853\n",
      "7864\n",
      "7892\n",
      "7918\n",
      "7941\n",
      "7946\n",
      "7954\n",
      "7961\n",
      "7970\n",
      "7984\n",
      "7992\n",
      "8015\n",
      "8027\n",
      "8044\n",
      "8053\n",
      "8056\n",
      "8127\n",
      "8142\n",
      "8161\n",
      "8163\n",
      "8168\n",
      "8174\n",
      "8176\n",
      "8179\n",
      "8184\n",
      "8193\n",
      "8198\n",
      "8225\n",
      "8228\n",
      "8236\n",
      "8245\n",
      "8266\n",
      "8271\n",
      "8275\n",
      "8285\n",
      "8335\n",
      "8337\n",
      "8357\n",
      "8365\n",
      "8443\n",
      "8457\n",
      "8465\n",
      "8503\n",
      "8509\n",
      "8517\n",
      "8521\n",
      "8529\n",
      "8532\n",
      "8556\n",
      "8558\n",
      "8562\n",
      "8599\n",
      "8605\n",
      "8624\n",
      "8640\n",
      "8644\n",
      "8656\n",
      "8661\n",
      "8662\n",
      "8706\n",
      "8707\n",
      "8715\n",
      "8730\n",
      "8736\n",
      "8758\n",
      "8791\n",
      "8827\n",
      "8834\n",
      "8879\n",
      "8882\n",
      "8884\n",
      "8943\n",
      "8954\n",
      "8957\n",
      "9014\n",
      "9029\n",
      "9039\n",
      "9040\n",
      "9056\n",
      "9070\n",
      "9150\n",
      "9177\n",
      "9180\n",
      "9182\n",
      "9200\n",
      "9227\n",
      "9252\n",
      "9315\n",
      "9338\n",
      "9342\n",
      "9377\n",
      "9403\n",
      "9415\n",
      "9417\n",
      "9420\n",
      "9435\n",
      "9456\n",
      "9469\n",
      "9477\n",
      "9480\n",
      "9481\n",
      "9503\n",
      "9514\n",
      "9517\n",
      "9544\n",
      "9549\n",
      "9567\n",
      "9597\n",
      "9598\n",
      "9603\n",
      "9648\n",
      "9689\n",
      "9699\n",
      "9719\n",
      "9721\n",
      "9723\n",
      "9731\n",
      "9753\n",
      "9775\n",
      "9778\n",
      "9836\n",
      "9857\n",
      "9874\n",
      "9891\n",
      "9898\n",
      "9913\n",
      "9915\n",
      "9946\n",
      "9951\n",
      "9956\n",
      "9960\n",
      "9966\n",
      "10009\n",
      "10016\n",
      "10020\n",
      "10024\n",
      "10032\n",
      "10041\n",
      "10072\n",
      "10088\n",
      "10105\n",
      "10106\n",
      "10134\n",
      "10148\n",
      "10179\n",
      "10193\n",
      "10218\n",
      "10269\n",
      "10278\n",
      "10281\n",
      "10300\n",
      "10303\n",
      "10317\n",
      "10318\n",
      "10368\n",
      "10377\n",
      "10380\n",
      "10383\n",
      "10385\n",
      "10386\n",
      "10477\n",
      "10482\n",
      "10491\n",
      "10507\n",
      "10511\n",
      "10558\n",
      "10572\n",
      "10575\n",
      "10600\n",
      "10601\n",
      "10609\n",
      "10618\n",
      "10628\n",
      "10644\n",
      "10657\n",
      "10668\n",
      "10691\n",
      "10702\n",
      "10729\n",
      "10731\n",
      "10737\n",
      "10743\n",
      "10746\n",
      "10774\n",
      "10791\n",
      "10892\n",
      "10920\n",
      "10945\n",
      "10949\n",
      "10958\n",
      "10976\n",
      "10995\n",
      "11014\n",
      "11096\n",
      "11102\n",
      "11139\n",
      "11153\n",
      "11155\n",
      "11160\n",
      "11189\n",
      "11224\n",
      "11235\n",
      "11252\n",
      "11261\n",
      "11300\n",
      "11320\n",
      "11328\n",
      "11334\n",
      "11356\n",
      "11380\n",
      "11384\n",
      "11387\n",
      "11411\n",
      "11434\n",
      "11481\n",
      "11499\n",
      "11501\n",
      "11537\n",
      "11550\n",
      "11555\n",
      "11567\n",
      "11588\n",
      "11589\n",
      "11602\n",
      "11604\n",
      "11639\n",
      "11656\n",
      "11657\n",
      "11669\n",
      "11670\n",
      "11679\n",
      "11683\n",
      "11688\n",
      "11692\n",
      "11709\n",
      "11769\n",
      "11772\n",
      "11824\n",
      "11826\n",
      "11828\n",
      "11841\n",
      "11857\n",
      "11886\n",
      "11896\n",
      "11897\n",
      "11909\n",
      "11939\n",
      "11984\n",
      "11985\n",
      "11990\n",
      "11998\n",
      "12010\n",
      "12015\n",
      "12028\n",
      "12049\n",
      "12067\n",
      "12081\n",
      "12083\n",
      "12090\n",
      "12094\n",
      "12144\n",
      "12155\n",
      "12176\n",
      "12205\n",
      "12206\n",
      "12214\n",
      "12238\n",
      "12243\n",
      "12265\n",
      "12267\n",
      "12285\n",
      "12288\n",
      "12289\n",
      "12295\n",
      "12305\n",
      "12306\n",
      "12315\n",
      "12336\n",
      "12362\n",
      "12370\n",
      "12398\n",
      "12445\n",
      "12456\n",
      "12471\n",
      "12479\n",
      "12480\n",
      "12487\n",
      "12491\n"
     ]
    }
   ],
   "source": [
    "print(validation(model))\n",
    "for i in range(0,len(test_set)):\n",
    "    if(test(model,test_set[i][0])==test_set[i][1][0].item()):\n",
    "        print(i)\n",
    "    #print(test(model,test_set[i][0]),test_set[i][1][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 3],\n",
      "        [0, 5, 3],\n",
      "        [5, 1, 3]]) tensor([2, 2, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(get_accuracy(torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m3\u001b[39;49m],[\u001b[39m5\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m]]),torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m2\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m1\u001b[39;49m])))\n",
      "Cell \u001b[1;32mIn[86], line 14\u001b[0m, in \u001b[0;36mget_accuracy\u001b[1;34m(y_hat, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m y\u001b[39m=\u001b[39my\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> 14\u001b[0m y_hat\u001b[39m=\u001b[39m[row\u001b[39m.\u001b[39;49mindex(\u001b[39mmax\u001b[39;49m(row)) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m y_hat]\n\u001b[0;32m     15\u001b[0m corrects\u001b[39m=\u001b[39m(y_hat\u001b[39m==\u001b[39my)\u001b[39m.\u001b[39msum()\n\u001b[0;32m     16\u001b[0m accuracy\u001b[39m=\u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m*\u001b[39mcorrects\u001b[39m/\u001b[39my_hat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[86], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m y\u001b[39m=\u001b[39my\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> 14\u001b[0m y_hat\u001b[39m=\u001b[39m[row\u001b[39m.\u001b[39;49mindex(\u001b[39mmax\u001b[39m(row)) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m y_hat]\n\u001b[0;32m     15\u001b[0m corrects\u001b[39m=\u001b[39m(y_hat\u001b[39m==\u001b[39my)\u001b[39m.\u001b[39msum()\n\u001b[0;32m     16\u001b[0m accuracy\u001b[39m=\u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m*\u001b[39mcorrects\u001b[39m/\u001b[39my_hat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(torch.tensor([[0,1,3],[0,5,3],[5,1,3]]),torch.tensor([2,2,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
